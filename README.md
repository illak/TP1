## Autor: Illak Zapata
---
# TP1 - Coderhouse - DE-FLEX 

Se recomienda acceder a la versi贸n de colab ya que explica el paso a paso del proyecto:

==============<br>
 [ENLACE A COLAB](https://colab.research.google.com/drive/1pVPXV6G2QoeSIrqzPC1qktQLt9797npE?usp=sharing) <br>
==============<br>

mientras que el archivo *TP1_local.ipynb* contiene el c贸digo necesario y adaptado para correr el proyecto de manera local.

Notar que para la versi贸n local se requieren las siguientes variables de entorno (en un archivo `.env`):

```
NEWSAPI_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_HOST=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_POST=XXXX
REDSHIFT_USER=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_PASS=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_DATABASE=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

La tabla creada en redshift se llama `df_news_api_dw` y se us贸 el siguiente script para su construcci贸n:

```sql
delete from i_zapata1989_coderhouse.df_news_api_dw;


CREATE TABLE i_zapata1989_coderhouse.df_news_api_dw (
	idrecord bigint identity(1,1),
	author varchar(256),
	content varchar(1000),
	description varchar(1000),
	publishedat timestamp,
	title varchar(1000),
	url varchar(1000),
	urltoimage varchar(1000),
	source_name varchar(256),
	source_id varchar(256),
	primary key(idRecord)
) distkey(source_name) sortkey(publishedat);
```

## Descripci贸n breve del proyecto
Se eligi贸 la api [NewsAPI](https://newsapi.org/), la cual nos permite obtener miles de art铆culos de miles de fuentes sobre un tema en particular y en un periodo de tiempo espec铆fico.
La API cuenta con una versi贸n gratuita que resulta suficiente para lograr el objetivo del entregable. Me pareci贸 una opci贸n interesante ya que los datos contienen una columna temporal que
permitir铆a hacer an谩lisis en un periodo determinado de tiempo. Por otra parte contiene mucho dato *"textual"*, lo cual habilita a proyectos de *"text mining"*, en este caso, de **noticias**.

Algunas consideraciones:

- Se separ贸 el proceso de **extracci贸n** del de **carga** de datos, esto es:
  - la creaci贸n de la tabla para DW se hizo usando *DBeaver*
  - mientras que el proceso de obtenci贸n de datos desde la API y su posterior carga se realiz贸 con Python (librerias `requests` + `pyspark`, entre otras)

![data flow diagram](data-flow-diagram-v2.png)


---

# TP2 - Coderhouse - DE-FLEX 

Para esta segunda entrega se pide que luego de la extracci贸n de datos desde la API realizada en la anterior entrega, estos datos deben tener un "tratamiento" o **limpieza** y posteriormente
cargarlos en Redshift. Para esto se utiliz贸 PySpark con el objetivo de que el procesamiento pueda escalar si los datos que se traen desde la API aumentan en t茅rminos de **volumen**.

Se recomienda acceder a la versi贸n de colab ya que explica el paso a paso del proyecto:

==============<br>
 [ENLACE A COLAB](https://colab.research.google.com/drive/1pVPXV6G2QoeSIrqzPC1qktQLt9797npE#scrollTo=o4ceD9aeuY6w) <br>
==============<br>
