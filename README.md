## Autor: Illak Zapata
---
# TP1 - Coderhouse - DE-FLEX 

Se recomienda acceder a la versi贸n de colab ya que explica el paso a paso del proyecto:

==============<br>
 [ENLACE A COLAB](https://colab.research.google.com/drive/1pVPXV6G2QoeSIrqzPC1qktQLt9797npE?usp=sharing) <br>
==============<br>

mientras que el archivo *TP1_local.ipynb* contiene el c贸digo necesario y adaptado para correr el proyecto de manera local.

Notar que para la versi贸n local se requieren las siguientes variables de entorno (en un archivo `.env`):

```
NEWSAPI_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_HOST=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_POST=XXXX
REDSHIFT_USER=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_PASS=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
REDSHIFT_DATABASE=XXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

La tabla creada en redshift se llama `df_news_api_dw` y se us贸 el siguiente script para su construcci贸n:

```sql
delete from i_zapata1989_coderhouse.df_news_api_dw;


CREATE TABLE i_zapata1989_coderhouse.df_news_api_dw (
	idrecord bigint identity(1,1),
	author varchar(256),
	content varchar(1000),
	description varchar(1000),
	publishedat timestamp,
	title varchar(1000),
	url varchar(1000),
	urltoimage varchar(1000),
	source_name varchar(256),
	source_id varchar(256),
	primary key(idRecord)
) distkey(source_name) sortkey(publishedat);
```

## Descripci贸n breve del proyecto
Se eligi贸 la api [NewsAPI](https://newsapi.org/), la cual nos permite obtener miles de art铆culos de miles de fuentes sobre un tema en particular y en un periodo de tiempo espec铆fico.
La API cuenta con una versi贸n gratuita que resulta suficiente para lograr el objetivo del entregable. Me pareci贸 una opci贸n interesante ya que los datos contienen una columna temporal que
permitir铆a hacer an谩lisis en un periodo determinado de tiempo. Por otra parte contiene mucho dato *"textual"*, lo cual habilita a proyectos de *"text mining"*, en este caso, de **noticias**.

Algunas consideraciones:

- Se separ贸 el proceso de **extracci贸n** del de **carga** de datos, esto es:
  - la creaci贸n de la tabla para DW se hizo usando *DBeaver*
  - mientras que el proceso de obtenci贸n de datos desde la API y su posterior carga se realiz贸 con Python (librerias `requests` + `pyspark`, entre otras)

![data flow diagram](data-flow-diagram-v2.png)


---

# TP2 - Coderhouse - DE-FLEX 

Para esta segunda entrega se pide que luego de la extracci贸n de datos desde la API realizada en la anterior entrega, estos datos deben tener un "tratamiento" o **limpieza** y posteriormente
cargarlos en Redshift. Para esto se utiliz贸 PySpark con el objetivo de que el procesamiento pueda escalar si los datos que se traen desde la API aumentan en t茅rminos de **volumen**.

Se recomienda acceder a la versi贸n de colab ya que explica el paso a paso del proyecto:

==============<br>
 [ENLACE A COLAB](https://colab.research.google.com/drive/1pVPXV6G2QoeSIrqzPC1qktQLt9797npE#scrollTo=o4ceD9aeuY6w) <br>
==============<br>


---
# TP3 - Coderhouse - DE-FLEX 
Para esta tercer entrega se debe **portar** las dos etapas anteriores a [Apache Airflow](https://airflow.apache.org/). Para esto se va a utilizar Docker, es decir, vamos a hacer el proyecto "portable", de esta manera podr谩 ejecutarse desde casi cualquier sistema y sin tanto trabajo. Como requisito se requiere tener instalado **Docker** y **Docker Compose**.

El proyecto se encuentra en la carpeta *[entregable_3_airflow](https://github.com/illak/TP_DE_FLEX_CODER/tree/master/entregable_3_airflow)*.

El directorio dag tiene la siguiente estructura:

```
.
 .airflowignore
 ETL_news_dag.py
 __init__.py
 pyscripts
     .env
     ETL_newsapi.py
     jars
         postgresql-42.2.27.jre7.jar
```

En donde:

* `ETL_news_dag.py` contiene el DAG que *orquesta* todo el proceso de *Extract + Transform + Load* de los datos.
* `ETL_newsapi.py` en el directorio `pyscripts` contiene c贸digo que b谩sicamente realiza las tareas de los entregables 1 y 2. Este archivo no es registrado como DAG por el "scheduler" de airflow ya que se encuentra en el archivo `.airflowignore`.
* Es necesario completar el archivo `.env` con las credenciales necesarias. Para esto se debe seguir el modelo del archivo `.env_modelo` que se encuentra en en directorio de `pyscripts`.

A continuaci贸n se muestra el diagrama del flujo de datos y las tareas que orquesta Airflow:

![data flow diagrama 3er entregable](diagrama_entregable_3.png)


## Para hacer el deploy:

Dentro del directorio del entregable 3:

```
$docker compose up --build
```

---
# TP4 - Final - Coderhouse - DE-FLEX 

Para la entrega final se pide que, en base a lo realizado en la entrega anterior, se agreguen **alertas**. Estas alertas deber谩n enviarse mediante **mail**.

Para nuestro proyecto se plante贸 el env铆o de alertas en 3 situaciones:

- Ejecuci贸n exitosa: Para este caso se env铆a un mail cada vez que una tarea se realiza de manera correcta.
- Ejecuci贸n fallida (`email_on_failure`): Para este caso se env铆a un mail cada vez que una tarea falla en su ejecuci贸n.
- Reintento de ejecuci贸n (`email_on_retry`): Para este caso se env铆a un mail cada vez que se reintenta la ejecuci贸n de una tarea.

Adem谩s se hace uso del sistema de _"thresholds"_, con la finalidad de seguir un control sobre la obtenci贸n de datos desde la API.
Para este caso se hace uso de la variable ```min_regs```, como se muestra en la siguiente captura:

![configuraci贸n de variable de threshold](tp_final_variables.png)

Esta variable controla la cantidad m铆nima de registros que se esperan de la etapa (o tarea) **EXTRACT** de datos (extracci贸n). En este sentido, si se setea la variable en 150 registros y la cantidad de noticias obtenidas se encuentra por debajo de este valor, se procede con el env铆o de un mail informando de esta situaci贸n.
A continuaci贸n podemos observar la estructura de este **DAG**:

![configuraci贸n de variable de threshold](dags_tp_final.png)

Se puede observar que ahora tenemos una tarea intermedia ```check_num_rows``` que realiza el chequeo de la condici贸n de cantidad de registros o filas. En el caso en que la condici贸n se cumpla de manera positiva, se procede con la tarea de **Transformaci贸n y Carga** (TRANSFORM & LOAD).

Es necesario configurar las variables de entorno en el archivo `env_keys.env` de la forma en que se muestra en el modelo de archivo `env_keys.env_modelo`:

```
AIRFLOW__SMTP__SMTP_MAIL_FROM='xxxxxxxx@gmail.com'
AIRFLOW__SMTP__SMTP_USER='xxxxxxxx@gmail.com'
AIRFLOW__SMTP__SMTP_PASSWORD='xxxxxxxxxxxxxxxxxx'
AIRFLOW__SMTP__SMTP_HOST='smtp.gmail.com'
AIRFLOW__SMTP__SMTP_PORT='587'
AIRFLOW_VAR_MAIL: 'xxxxxxxx@gmail.com'
AIRFLOW_VAR_MAIL_PASS: 'xxxxxxxxxxxxxxxxxx'
```

Para el env铆o de mails autom谩ticos de tareas realizadas se usan:
* AIRFLOW__SMTP__SMTP_MAIL_FROM
* AIRFLOW__SMTP__SMTP_USER
* AIRFLOW__SMTP__SMTP_PASSWORD
* AIRFLOW__SMTP__SMTP_HOST
* AIRFLOW__SMTP__SMTP_PORT

Los cuales se usan con los mecanismos que provee Airflow.

Para el env铆o de mails en base a **thresholds** se usan:
* AIRFLOW_VAR_MAIL
* AIRFLOW_VAR_MAIL_PASS

Estos 煤ltimos se usan en conjunto con la libreria ```smtplib```, para env铆os m谩s personalizados. 

En todos los casos, para el env铆o de mails autom谩ticos, se hace uso del **smtp** de gmail. 

Adem谩s debemos configurar las variables de entorno como se explic贸 en la entrega anterior (necesarias para el uso de la API de noticias y de REDSHIFT).

El proyecto se encuentra en la carpeta *[EntregaFinal_IllakZapata_DATENG_51935](https://github.com/illak/TP_DE_FLEX_CODER/tree/master/EntregaFinal_IllakZapata_DATENG_51935)*. Dentro del directorio y luego de haber configurado todo lo relacionado a variables, levantamos el servicio con el comando:

```
$docker compose up --build
```